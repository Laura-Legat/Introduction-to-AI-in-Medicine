{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMSITdefOJCkRpwZkPhodhK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["This notebook has been adapted and inspired by the following sources:\n","\n","*    This [notebook](https://colab.research.google.com/github/ccc-frankfurt/Practical_ML_SS21/blob/main/week06/Shakespeare_Poetry_Generation_RNN_LSTM_solution.ipynb) on poetry generation using an RNN\n","*   This [post](https://machinelearningmastery.com/text-generation-with-lstm-in-pytorch/) about text generation with LSTM"],"metadata":{"id":"7u7LCoONgoAZ"}},{"cell_type":"markdown","source":["## Becoming Shakespeare with the Long Short Term Memory\n","\n","In this notebook, you will learn how to generate sonnets like Shakespeare himself by training a so-called Long Short Term Memory (LSTM) model. A LSTM is a type of recurrent neural network (RNN), another different neural network architecture commonly used to learn and generate sequential data, such as natural text, chemical strings, DNA etc. In RNNs, the order of the elements is important, and the LSTM cell contains an internal memory, or state, of the past, enabling them to predict the next element based on that state. The vanilla RNN suffers from a few disadvantages, among which the so-called [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), which led to the creation of the LSTM - a robust architecture which is suitable for long-term dependency understanding and does not suffer from quadratic complexity like the [Transformer](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)) architecture.\n","\n","Like we did in the past notebooks, we will define our network structure, this time as a Python class for our LSTM, and then train/evaluate using a curated dataset of Shakespeare's sonnets. Finally, we will use our trained LSTM to generate text similar to the sonnets, and try to get good at imitating Shakespeare's writing style. As a first step, we import our dependencies:"],"metadata":{"id":"t8KHg85QfF61"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"uGWB26fBg7L9","executionInfo":{"status":"ok","timestamp":1745091344078,"user_tz":-120,"elapsed":8708,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["And we set our device to either CPU or GPU:"],"metadata":{"id":"0F9JFSmEipRB"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('Device: ', device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00OP8oeIiouz","executionInfo":{"status":"ok","timestamp":1745091357540,"user_tz":-120,"elapsed":20,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"21d2d9a6-b64d-4476-af7c-10a35fb84e0f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Device:  cuda\n"]}]},{"cell_type":"markdown","source":["## The data: Shakespeare's sonnets\n","\n","Shakespeare's sonnets can be found through the following URL: http://shakespeare.mit.edu/\n","\n","The authors of [this notebook](https://colab.research.google.com/github/ccc-frankfurt/Practical_ML_SS21/blob/main/week06/Shakespeare_Poetry_Generation_RNN_LSTM_solution.ipynb#scrollTo=stneSw5L77Ln) have extracted all the plain text of the sonnets here: https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt  \n","\n","We will thus download it from there."],"metadata":{"id":"CxsUrDJtiyuC"}},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_SS21/master/week06/sonnets.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MoA_TsNsjExC","executionInfo":{"status":"ok","timestamp":1745091362378,"user_tz":-120,"elapsed":311,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"fe359f69-9c7a-4bc1-d5ca-3636cfbffddb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-04-19 19:36:02--  https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_SS21/master/week06/sonnets.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 94081 (92K) [text/plain]\n","Saving to: ‘sonnets.txt’\n","\n","sonnets.txt         100%[===================>]  91.88K  --.-KB/s    in 0.02s   \n","\n","2025-04-19 19:36:03 (5.72 MB/s) - ‘sonnets.txt’ saved [94081/94081]\n","\n"]}]},{"cell_type":"markdown","source":["Let's observe and verify parts of the sonnets:"],"metadata":{"id":"WvBtkPBjjQ6a"}},{"cell_type":"code","source":["# open the downloaded file in r=read mode\n","with open('sonnets.txt', 'r') as f:\n","    text = f.read()\n","\n","# print an excerpt of the text\n","print(text[:128])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0qswznuajUw8","executionInfo":{"status":"ok","timestamp":1745092273935,"user_tz":-120,"elapsed":19,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"34327351-c666-4737-f63b-ef8d63250fb5"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["From fairest creatures we desire increase,\n","That thereby beauty's rose might never die,\n","But as the riper should by time decease,\n","\n"]}]},{"cell_type":"markdown","source":["### Generating character encodings"],"metadata":{"id":"_f2XkZp5l8lq"}},{"cell_type":"markdown","source":["To be able to feed text into RNNs, we first have to choose a good representation, meaning an abstraction of the text into numeric format, since that is what the RNN expects. There are several options to do so ranging from simpler character embeddings to more sophisticated approaches like [word embeddings](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa) and more.\n","\n","We will use character encodings in this notebook, which work as follows: First we define an alphabet, aka a set of characters that we want to be able to represent. An alphabet could be all letters from A-Z, or include numerical characters, or special tokens. Then, we create a one-hot vector for each character, where the length of each one is equal to the size of our alphabet, and the \"hot\" position indicates the character we want to represent. For instance, assume our alphabet consists only of ABC. Then, the one-hot vector for C would be [0,0,1], the one-hot vector for A [1,0,0] if we assume the positions to be [A,B,C].\n","\n","For simplicity, we define our alphabet here as \"all unique letters in our dataset\"."],"metadata":{"id":"3MYwFFlqjjK7"}},{"cell_type":"code","source":["# set our alphabet to be all characters in the text\n","chars = tuple(set(text))\n","\n","# convert character data into integers by simply mapping each character to the corresponding integer\n","int2char = dict(enumerate(chars))\n","char2int = {ch: ii for ii, ch in int2char.items()}\n","\n","# encode the complete text\n","encoded_text = np.array([char2int[ch] for ch in text])\n","\n","#showing the textual and encoded excerpt\n","print(text[:128])\n","print(encoded_text[:128])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tNtPivcZjsAt","executionInfo":{"status":"ok","timestamp":1745092287840,"user_tz":-120,"elapsed":45,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"2d807123-662b-41ff-ed53-b7b76761f85e"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["From fairest creatures we desire increase,\n","That thereby beauty's rose might never die,\n","But as the riper should by time decease,\n","\n","[50  2  3 45 30 59  1 13  2  6 26 15 30  4  2  6  1 15 49  2  6 26 30  8\n","  6 30  9  6 26 13  2  6 30 13 23  4  2  6  1 26  6 27 28 33 60  1 15 30\n"," 15 60  6  2  6 19 57 30 19  6  1 49 15 57 38 26 30  2  3 26  6 30 45 13\n"," 39 60 15 30 23  6 14  6  2 30  9 13  6 27 28 41 49 15 30  1 26 30 15 60\n","  6 30  2 13 40  6  2 30 26 60  3 49 24  9 30 19 57 30 15 13 45  6 30  9\n","  6  4  6  1 26  6 27 28]\n"]}]},{"cell_type":"code","source":["char2int['F']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KweYrPavnTxz","executionInfo":{"status":"ok","timestamp":1745092444849,"user_tz":-120,"elapsed":25,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"70b29fb1-2e0d-4835-82d2-6a614899429c"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["As you see, the uppercase F has been mapped to the integer 50, and 50 is also the first index to appear in our encoded sequence. We can also observe the total text length of our dataset, and how many unique characters our alphabet contains:"],"metadata":{"id":"MItCeqYEnZCr"}},{"cell_type":"code","source":["print('Total characters: ', len(text))\n","print('Vocabulary size: ', len(chars))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrSpqaCVoJwE","executionInfo":{"status":"ok","timestamp":1745092695470,"user_tz":-120,"elapsed":8,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"8a9f9743-8a51-4cb9-aa94-125dd0b8dfb0"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Total characters:  94081\n","Vocabulary size:  61\n"]}]},{"cell_type":"markdown","source":["Next, we need to think about our goal, which is to generate text similar to Shakepeare's sonnets. Since we use a character-based approach, we can simplify our goal down to \"we want to predict the next character given the past characters\", where \"the past characters\" is a defined look-back window, e.g. predicting the next character based on the past 100 characters before it. That is, with character 1 to 100 as input, the LSTM is going to predict character 101. For example, imagine a look-back window of length 3:\n","\n","hel -> l\n","\n","ell -> o\n","\n","llo -> !\n","\n","and so on. We can frame this as a classification problem: given a look-back window, predict the next most likely class (letter) our of all possible classes (alphabet). Hence, our training data will consist of fixed-sized look-back windows of text, and the labels will be the next characters, given the corresponding window. Thus, we need to split our whole sonnet textual data into such windows."],"metadata":{"id":"v0l5ALZaog2j"}},{"cell_type":"markdown","source":["### Creating the dataset"],"metadata":{"id":"xId-L2B-pyr7"}},{"cell_type":"code","source":["window_size = 100 # look-back window length, feel free to play around with this value\n","windows_x = []\n","windows_y = []\n","for i in range(0, len(text) - seq_length, 1):\n","    seq_in = raw_text[i:i + seq_length]\n","    seq_out = raw_text[i + seq_length]\n","    dataX.append([char_to_int[char] for char in seq_in])\n","    dataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","print(\"Total Patterns: \", n_patterns)"],"metadata":{"id":"NJLsVsv3pxJM"},"execution_count":null,"outputs":[]}]}