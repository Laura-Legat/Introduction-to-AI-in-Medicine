{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gfa2t8xuVGV6"
   },
   "source": [
    "*Copyright statement:\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.*\n",
    "\n",
    "# Polynomial Regression\n",
    "\n",
    "The goal of polynomial regression (and also linear regression) is to find the function which best models the relationship between an independent variable and a dependent variable as an degree-$d$ polynomial. With polynomial regression, this means finding the coefficients that provide the best fit to a non-linear relationship in the data, which a straight line (linear regression) cannot always do precisely.\n",
    "\n",
    "Imagine that $y$ is our true underlying polynomial, which we often do not know. However, we might know data points $x$ sampled around our true polynomial, and we want to find the polynomial $\\hat y$ that approximates $y$ given our data. In polynomial regression, we can model this relationship as:\n",
    "\n",
    "$$y \\approx \\hat y = \\theta_0 x_p^0 + \\theta_1 x_p^1 + \\theta_2 x_p^2 + \\theta_3 x_p^3 + ... + \\theta_d x_p^d$$\n",
    "\n",
    "where the thetas $\\theta_0, \\theta_1, ..., \\theta_d$ are the parameters/weights we are trying to find and the $x_p$ are polynomial feature vectors $\\in \\mathbb{R}^{n}$ where $n$ is the number of sampled datapoints. We can also use matrix notation and summarize our parameters and our polynomial features as matrix-vector multiplication:\n",
    "\n",
    "$$y \\approx \\hat y = X_{poly} \\mathbf{\\theta}$$\n",
    "\n",
    "Where $X_{poly} \\in \\mathbb{R}^{n \\times d+1}$ is the polynomial feature matrix and $\\mathbf{\\theta}$ are the weights. Imagine you are trying to fit a degree-3 polynomial (i.e. $\\theta_0 x^0 + \\theta_1 x^1 + \\theta_2 x^2 + \\theta_3 x^3$) to $n=3$ datapoints $x = [2, 3, 5]$. The polynomial feature matrix then expands the input vector $x$ into multiple features like so:\n",
    "\n",
    "$$X_{poly} = \\begin{bmatrix}\n",
    "2^0 & 2^1 & 2^2 & 2^3 \\\\\n",
    "3^0 & 3^1 & 3^2 & 3^3 \\\\\n",
    "5^0 & 5^1 & 5^2 & 5^3\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "hence a $(n \\times d+1)$ aka a $(3 \\times 4)$ matrix, since we are factoring in the x's to the power of 0 as well. The weight vector $\\theta$ is of dimensionality $(d + 1 \\times 1)$ after making it a column vector, hence we get the model's output dimensionality of $(3 \\times 4) * (4 \\times 1) = (3 \\times 1)$, which makes sense considering we have the same input dimensions.\n",
    "\n",
    "Throughout this notebook, we will:\n",
    "\n",
    "*    Generate synthetic, noisy data we are trying to fit a function to\n",
    "*    Create a polynomial feature matrix to make a linear model fit non-linear data\n",
    "*    Finding the optimal weights through least squares\n",
    "*    Evaluating the generalization error and empirical risk through the Mean Squared Error\n",
    "* Intercatively visualize the impact of different settings\n",
    "\n",
    "We provide some intermediate checks to make sure your outputs are correct so you can move on to the next task. First, we import all necessary packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3751,
     "status": "ok",
     "timestamp": 1745827168450,
     "user": {
      "displayName": "Laura Legat",
      "userId": "08447403520972321571"
     },
     "user_tz": -120
    },
    "id": "Zor1w797aEwF",
    "outputId": "8a408251-8514-4bdc-9caf-55de1ededda5"
   },
   "outputs": [],
   "source": [
    "!pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "error",
     "timestamp": 1745827175421,
     "user": {
      "displayName": "Laura Legat",
      "userId": "08447403520972321571"
     },
     "user_tz": -120
    },
    "id": "-eUu9umwTLbW",
    "outputId": "c5cf9f1a-962d-47c3-a306-4a2eba1a7bcf"
   },
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ipywidgets import interact\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WERNmNpOXwSc"
   },
   "source": [
    "Next, we set a seed for deterministic data generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1745827119144,
     "user": {
      "displayName": "Laura Legat",
      "userId": "08447403520972321571"
     },
     "user_tz": -120
    },
    "id": "GmzKenNPV3vB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb6fc883e90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVhW56_RZnju"
   },
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37EKR3KtX_jG"
   },
   "source": [
    "Now let us generate our data. We define the sine function as our true underlying function and generate some datapoints around it. To make it look more like real data you could have gained from an experiment, we add some Gaussian noise as well. In general, training on realistic/noisy data also makes sure that our model becomes more robust and can generalize even if outliers are present, as it helps simulate the unpredictability of the real world.\n",
    "\n",
    "We first create a function `true_polynomial` which models our sine function, aka our true polynomial $y$. Here $x \\in \\mathbb{R}$ is expected to be between 0 and 1, so we obtain a sine wave that completes a full period ($2 \\pi$) between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_polynomial(x):\n",
    "    return torch.sin(2 * torch.pi * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide the functionality for generating datapoints $x$ through `generate_datapoints`, which takes in `n_samples` the number of datapoints we want to create around our true polynomial, and `noise` the degree of noise in our data, aka the spread of our data around the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datapoints(n_samples, noise):\n",
    "    torch.manual_seed(seed)\n",
    "    X = torch.rand(n_samples, 1) # n_samples in [0,1)\n",
    "    # obtain noisy observations around the true polynomial\n",
    "    noisy_y = noise * torch.randn(n_samples) # generate gaussian noise \n",
    "    y = true_polynomial(X.squeeze()) + noisy_y # y = y + noise\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement polynomial feature expansion\n",
    "\n",
    "As previously mentioned, our relationship is of the form:\n",
    "\n",
    "$$y \\approx \\hat y = \\theta_0 x_p^0 + \\theta_1 x_p^1 + \\theta_2 x_p^2 + \\theta_3 x_p^3 + ... + \\theta_d x_p^d = X_{poly} \\mathbf{\\theta}$$\n",
    "\n",
    "However, linear regression can only fit a straight line. Thus, if we have sampled datapoints following a non-linear pattern, like curved data or a parabola, we need a polynomial regression model that solves this problem by adding powers of $x$ as new inputs. That way, the model can bend to fit more complex shapes of relationships in the data. For this to work, we first need to transform our inputs $x$ to polynomial feature matrix $X_{poly}$. With this, we transform our problem such that a linear model can actually fit non-linear data. In `scikit-learn` for example, this is done through [`Polynomial Features`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html), however you will implement this manually.\n",
    "\n",
    "Given an $(n \\times 1)$-dimensional input tensor `x` as well as the scalar `degree` $d$, generate a new feature matrix consisting of all polynomial combintions of $x$, aka the matrix $X_{poly}$. The function should return a $(n \\times d+1)$-dimensional matrix of type `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_polynomial_features(x, degree):\n",
    "    #TODO: implement polynomial feature expansion\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing build_polynomial_features\n",
    "x_test, y_test = generate_datapoints(5, 0.25)\n",
    "X_poly_test = build_polynomial_features(x_test, 3)\n",
    "\n",
    "expected_X_poly_test = torch.tensor([[1.0000, 0.8823, 0.7784, 0.6868],\n",
    "                                [1.0000, 0.9150, 0.8372, 0.7661],\n",
    "                                [1.0000, 0.3829, 0.1466, 0.0561],\n",
    "                                [1.0000, 0.9593, 0.9203, 0.8828],\n",
    "                                [1.0000, 0.3904, 0.1524, 0.0595]])\n",
    "\n",
    "assert X_poly_test.shape == torch.Size([5, 4])\n",
    "assert type(X_poly_test) == torch.Tensor\n",
    "assert torch.allclose(X_poly_test, expected_X_poly_test, atol=1e-3) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Fitting the model\n",
    "\n",
    "Now that you have expanded the input features to their polynomial features, you now want to find the best weight vector $\\mathbf{\\theta}$, such that multiplying it with our polynomial features comes as close as possible to our true outputs $y$. One of the ways we can solve this problem is the Least Squares method, which minimizes the squared errors between the predictions and real outputs. This is given by:\n",
    "\n",
    "$$\\mathbf{\\theta} = (X_{poly}^T X_{poly})^{-1} X_{poly}^T y$$\n",
    "\n",
    "where $X_{poly}$ is your polynomial feature matrix and $y$ are the true target values. This is also a closed-form solution to linear regression. In this task, you should implement the `fit_least_squares` function, which takes as arguments `X_poly`, your matrix of polynomial features, and the true values `y`. It should implement the above formula and return the optimal $\\theta$ vector of dimensionality $(d+1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_least_squares(X_poly, y):\n",
    "    #TODO implement least squares\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_theta_test = torch.tensor([64.6249, -295.1517,  402.4412, -172.2118])\n",
    "theta_test = fit_least_squares(X_poly_test, y_test)\n",
    "\n",
    "assert torch.allclose(theta_test, expected_theta_test, atol=1e-3) == True\n",
    "assert type(theta_test) == torch.Tensor\n",
    "assert theta_test.shape == torch.Size([4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Predict outputs\n",
    "\n",
    "Finally, using a linear approach, we predict our $\\hat y$ as:\n",
    "\n",
    "$$\\hat y = X_{poly} \\mathbf{\\theta}$$\n",
    "\n",
    "Implement the `predict` function below that does exactly this operation, given `X_poly` as the polynomial feature matrix, and `theta` as the weights $\\mathbf{\\theta}$, returning $\\hat y$ of dimensionality $(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_poly, theta):\n",
    "    # TODO: implement prediction here\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = predict(X_poly_test, theta_test)\n",
    "expected_y_hat_test = torch.tensor([-0.7863, -0.4297,  0.9489, -0.1939,  0.4849])\n",
    "\n",
    "assert y_hat_test.shape == torch.Size([5])\n",
    "assert type(y_hat_test) == torch.Tensor\n",
    "assert torch.allclose(y_hat_test, expected_y_hat_test, atol=1e-3) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Evaluating the model\n",
    "\n",
    "Finally, after you found the optimal weights and made predictions, it is time to measure how good these predictions are. This is what the Mean Squared Error (MSE) is for, which measures how far off your predictions are from the true output. Hence, a smaller MSE is better. We will use the MSE to calculate our empirical risk (error over our training set) and our generalization error (error over our test set).\n",
    "\n",
    "The MSE is measured as:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum^n_{i=1} (y_i - \\hat y_i)^2$$\n",
    "\n",
    "where $n$ is the number of datapoints, $y_i$ is the $i$'th true ouput and $\\hat y_i$ is the prediction the model makes for the $i$'th data point. Implement the MSE calculation in the `mean_squared_error` function. The function should return the full mean squared error as `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_hat):\n",
    "    # TODO: Implement MSE formula\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = mean_squared_error(y_test, y_hat_test)\n",
    "\n",
    "assert type(mse_test) == float\n",
    "assert torch.allclose(torch.tensor(mse_test), torch.tensor(0.03179203338161631), atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Below we provide a function `plot_polyfit`, which interactively puts your code snippets into a polynomial regression pipeline. It first generates some data with the specified amount of datapoints and noise, then uses `scikit-learn`'s `train_test_split` to split the generated data points into training and test set, builds the polynomial feature matrix out of $x$ and calculates the optimal weights $\\theta$. Finally, it predicts the $\\hat y$ and computes the training error and test errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polyfit(n_samples=30, noise=0.1, degree=3):\n",
    "    # generating data point\n",
    "    x, y = generate_datapoints(n_samples=n_samples, noise=noise)\n",
    "\n",
    "    # split data points into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x.numpy(), y.numpy(), test_size=0.2, random_state=seed)\n",
    "    X_train = torch.tensor(X_train)\n",
    "    X_test = torch.tensor(X_test)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    y_test = torch.tensor(y_test)\n",
    "\n",
    "    # building polynomial feature matrix from the X's\n",
    "    X_train_poly = build_polynomial_features(X_train, degree)\n",
    "    X_test_poly = build_polynomial_features(X_test, degree)\n",
    "    \n",
    "    # compute optimal weights via least squares method\n",
    "    weights = fit_least_squares(X_train_poly, y_train)\n",
    "    \n",
    "    # prepare predictions\n",
    "    x_plot = torch.linspace(0, 1, steps=100).unsqueeze(1)\n",
    "    x_plot_poly = build_polynomial_features(x_plot, degree)\n",
    "    y_plot = predict(x_plot_poly, weights)\n",
    "    y_train_pred = predict(X_train_poly, weights)\n",
    "    y_test_pred = predict(X_test_poly, weights)\n",
    "    \n",
    "    # compute emp risk and generalization error\n",
    "    train_error = mean_squared_error(y_train, y_train_pred)\n",
    "    test_error = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # plotting\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(X_train, y_train, label=\"Training data\", alpha=0.7)\n",
    "    plt.scatter(X_test, y_test, label=\"Test data\", color=\"orange\")\n",
    "    plt.plot(x_plot, y_plot.detach(), 'r-', label=f\"Polynomial degree {degree}\")\n",
    "    plt.plot(x_plot, true_polynomial(x_plot.squeeze()).detach().numpy(), label=\"True function\", color=\"green\", linestyle=\"dashed\")\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.legend()\n",
    "    plt.title(f\"Train error: {train_error:.3f} | Test (generalization) error: {test_error:.3f}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to execute the code below and play around with the sliders in oder to increase/decrease e.g. the number of datapoints as well as the degree of the polynomial that should be predicted to fit the data. You should notice that very low datasamples paired with a higher-degree polynomial will lead to so-called overfitting (try e.g. n_samples=15 and degree=11): While a very high-degree polynomial can fit most/all training set datapoints perfectly, its ability to generalize to the unseen test set points is very bad, leading to a small training error, but a high generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_polyfit, \n",
    "         n_samples=(5, 100, 5),\n",
    "         noise=(0.0, 0.7, 0.1),\n",
    "         degree=(0, 15, 1));"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMeSaAqHO4v44/ruLc47EnK",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "xAImed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
