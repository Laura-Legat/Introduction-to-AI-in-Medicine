{"cells":[{"cell_type":"markdown","id":"perceived-donor","metadata":{"id":"perceived-donor"},"source":["# Introduction to PyTorch\n","\n","This notebook was adapted from [Stanford's CS224N Pytorch](https://github.com/SunnyHaze/Stanford-CS224N-NLP/blob/main/CS224N%20PyTorch%20Tutorial.ipynb) Tutorial by Dilara Soylu as well as the official [PyTorch 60 Minute Blitz Tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) demo for PyTorch.\n","\n","We will have a basic introduction to `PyTorch` and Tensors and how to use them to create, train and evaluate Neural Networks. In the end, we will build, train, and evaluate our first classifier by classifying two moons!"]},{"cell_type":"markdown","id":"static-african","metadata":{"id":"static-african"},"source":["## Introduction\n","[PyTorch](https://pytorch.org/) is a machine learning framework that is used in both academia and industry for various applications. PyTorch started of as a more flexible alternative to [TensorFlow](https://www.tensorflow.org/), which is another popular machine learning framework. At the time of its release, `PyTorch` appealed to the users due to its user friendly nature: as opposed to defining static graphs before performing an operation as in `TensorFlow`, `PyTorch` allowed users to define their operations as they go, which is also the approached integrated by `TensorFlow` in its following releases. Although `TensorFlow` is more widely preferred in the industry, `PyTorch` is often times the preferred machine learning framework for researchers.\n","\n","Now that we have learned enough about the background of `PyTorch`, let's start by importing it into our notebook."]},{"cell_type":"code","execution_count":150,"id":"hindu-wales","metadata":{"id":"hindu-wales","executionInfo":{"status":"ok","timestamp":1744884038252,"user_tz":-120,"elapsed":2,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn # contains functionality for building neural networks\n","import numpy as np"]},{"cell_type":"markdown","source":["Like in the last notebook, we can use `__version__` to check the `PyTorch` version that Colab is running on."],"metadata":{"id":"B6S-n3L-_Ovm"},"id":"B6S-n3L-_Ovm"},{"cell_type":"code","source":["torch.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"g1iRWzuA_MGt","executionInfo":{"status":"ok","timestamp":1744884038283,"user_tz":-120,"elapsed":28,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"63d667be-6e33-49eb-9cb4-d47debb203d4"},"id":"g1iRWzuA_MGt","execution_count":151,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.6.0+cu124'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":151}]},{"cell_type":"markdown","id":"adjacent-hearing","metadata":{"id":"adjacent-hearing"},"source":["PyTorch is open source and the documentation can he accessed [here](https://pytorch.org/docs/stable/index.html). With it imported, we can get started!"]},{"cell_type":"markdown","source":["## Tensors\n","\n","Tensors are the most basic building blocks in `PyTorch`. Tensors are similar to matrices, but the have extra properties and they can represent higher dimensions. For example, an square RGB image with 256 pixels in both sides can be represented by a 3x256x256 tensor, where the first 3 dimensions represent the color channels RGB. In `PyTorch`, we often use tensors to encode the inputs and outputs of a neural network model, as well as the model's parameters, to a numeric format which can be understood by the architecture. Tensors can run on GPU's to accelerate e.g. network training."],"metadata":{"id":"5Bsa9mtl_f_v"},"id":"5Bsa9mtl_f_v"},{"cell_type":"markdown","source":["### Tensor Initialization"],"metadata":{"id":"RJTV9cfdAAah"},"id":"RJTV9cfdAAah"},{"cell_type":"markdown","source":["There are several ways to instantiate tensors:\n","\n","**Directly from data**\n","\n","Tensors can be created directly from data. The data type is\n","automatically inferred."],"metadata":{"id":"r-cHMa9jAEDG"},"id":"r-cHMa9jAEDG"},{"cell_type":"code","source":["data = [[1, 2], [3, 4]]\n","x_data = torch.tensor(data)\n","x_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZjm8c5YAgVu","executionInfo":{"status":"ok","timestamp":1744884038317,"user_tz":-120,"elapsed":35,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"8d85e36e-ed92-4e1d-b26d-6e629f962bd1"},"id":"vZjm8c5YAgVu","execution_count":152,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2],\n","        [3, 4]])"]},"metadata":{},"execution_count":152}]},{"cell_type":"code","source":["print(type(x_data)) # prints the type of the data structure, i.e. tensor\n","print(x_data.dtype) # print type of elements in the tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W1IRvL6CAk0P","executionInfo":{"status":"ok","timestamp":1744884038329,"user_tz":-120,"elapsed":14,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"16633cfc-38e9-4676-e383-3bb6711e8d0d"},"id":"W1IRvL6CAk0P","execution_count":153,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.Tensor'>\n","torch.int64\n"]}]},{"cell_type":"markdown","source":["We can also speficy the data type (`dtype`) directly:"],"metadata":{"id":"eAZf8_8wCCOn"},"id":"eAZf8_8wCCOn"},{"cell_type":"code","source":["# We are using the dtype to create a float tensor\n","x_float = torch.tensor(data, dtype=torch.float)\n","x_float.dtype"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LseoNwzFCIMo","executionInfo":{"status":"ok","timestamp":1744884038339,"user_tz":-120,"elapsed":12,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"b18d8ae7-1337-4837-b5d0-3a1f12c67a3c"},"id":"LseoNwzFCIMo","execution_count":154,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":154}]},{"cell_type":"markdown","source":["**From a Python List**\n","\n","We can initalize a tensor from a Python list, which could include sublists. The dimensions and the data types will be automatically inferred by PyTorch when we use torch.tensor()."],"metadata":{"id":"fQLpeHe9BPtn"},"id":"fQLpeHe9BPtn"},{"cell_type":"code","source":["# Initialize a tensor from a Python List\n","data = [\n","        [0, 1],\n","        [2, 3],\n","        [4, 5]\n","       ]\n","x_python = torch.tensor(data)\n","\n","# Print the tensor\n","x_python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"or-i0liiBYjP","executionInfo":{"status":"ok","timestamp":1744884038381,"user_tz":-120,"elapsed":41,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"9259d9ee-fbd6-4d81-f1a3-39737b1a143d"},"id":"or-i0liiBYjP","execution_count":155,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])"]},"metadata":{},"execution_count":155}]},{"cell_type":"markdown","source":["**From a NumPy array**\n","\n","Tensors can be created from NumPy arrays (and vice versa)."],"metadata":{"id":"t9GTBZTUAoiz"},"id":"t9GTBZTUAoiz"},{"cell_type":"code","source":["np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)\n","x_np"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2nb-2l9CAuIH","executionInfo":{"status":"ok","timestamp":1744884038390,"user_tz":-120,"elapsed":8,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"9335a76f-ba8b-4de0-f5ae-7f3d37580c5b"},"id":"2nb-2l9CAuIH","execution_count":156,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])"]},"metadata":{},"execution_count":156}]},{"cell_type":"markdown","source":["**From another tensor:**\n","\n","The new tensor retains the properties (shape, datatype) of the argument\n","tensor, unless explicitly overridden."],"metadata":{"id":"ZXF_6JMTA39e"},"id":"ZXF_6JMTA39e"},{"cell_type":"code","source":["x_ones = torch.ones_like(x_data) # retains the properties of x_data\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n","print(f\"Random Tensor: \\n {x_rand} \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LzdOoITTA6cO","executionInfo":{"status":"ok","timestamp":1744884038398,"user_tz":-120,"elapsed":9,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"1f4303a7-244b-4436-affa-045092ae39a4"},"id":"LzdOoITTA6cO","execution_count":157,"outputs":[{"output_type":"stream","name":"stdout","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.6109, 0.5822],\n","        [0.4923, 0.5586]]) \n","\n"]}]},{"cell_type":"markdown","source":["**With random or constant values:**\n","\n","Similar to what we have seen with NumPy, we can pre-fill tensors with static values like 1, or random numbers, just such that we have the shape as a placeholder. For this, we define `shape` as a tuple of tensor dimensions. In the functions below, it\n","determines the dimensionality of the output tensor."],"metadata":{"id":"uuMlNkLGCxWX"},"id":"uuMlNkLGCxWX"},{"cell_type":"code","source":["shape = (2, 3,) # 2x3x1 = 2x3 tensor\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n","\n","# to read the dimensions of a tensor, use shape or size()\n","print(zeros_tensor.shape)\n","print(zeros_tensor.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tzjbYjrwCyMw","executionInfo":{"status":"ok","timestamp":1744884038408,"user_tz":-120,"elapsed":11,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"b5826c9d-4851-44e5-bd1c-f5e6fc8d36e7"},"id":"tzjbYjrwCyMw","execution_count":158,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Tensor: \n"," tensor([[0.6514, 0.0606, 0.1859],\n","        [0.8758, 0.7169, 0.2399]]) \n","\n","Ones Tensor: \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n","\n","Zeros Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n","torch.Size([2, 3])\n","torch.Size([2, 3])\n"]}]},{"cell_type":"markdown","source":["### Tensor Attributes\n","\n","Tensors have several attributes which are important to know and adjust to your needs. Some of these properties are the aforementioned `shape` (aka dimensions), `dtype` (data type of the elements in the tensor) and the `device` they are stored on. The device could for instance be a CPU or a GPU. During training of neural networks, we might want to push our tensors onto the GPU device for accelerated training. Let's look at these tensor attributes below:"],"metadata":{"id":"xlOE3n1fDvG_"},"id":"xlOE3n1fDvG_"},{"cell_type":"code","source":["tensor = torch.rand(3, 4)\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cmN7lbeDELzY","executionInfo":{"status":"ok","timestamp":1744884038435,"user_tz":-120,"elapsed":25,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"dd69afe8-8aed-4ac0-cc77-76a76979cedd"},"id":"cmN7lbeDELzY","execution_count":159,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}]},{"cell_type":"markdown","source":["We can also index separate dimensions of our tensor. Here, we have a 2D tensor with 3 rows X 4 columns."],"metadata":{"id":"kyG-yS2yFR0v"},"id":"kyG-yS2yFR0v"},{"cell_type":"code","source":["print(tensor.shape[0]) # access the row dimension (is only considered the row dimension in 2D)\n","print(tensor.size(0)) # another way to access the row dimension"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"och1ze6IFrd6","executionInfo":{"status":"ok","timestamp":1744884038436,"user_tz":-120,"elapsed":14,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"c05bde3f-442e-4778-f1e8-85c38b93ea39"},"id":"och1ze6IFrd6","execution_count":160,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","3\n"]}]},{"cell_type":"markdown","source":["### Tensor Operations\n","\n","Over 100 tensor operations, including transposing, indexing, slicing,\n","mathematical operations, linear algebra, random sampling, and more are\n","comprehensively described\n","[here](https://pytorch.org/docs/stable/torch.html), where each of them can be run on the CPU and on the GPU.\n","\n","**Standard numpy-like indexing and slicing:**\n","\n","We index the rows of a 2D tensor by writing the row index into the parentheses. For indexing multiple dimensions, we use `:` and separate them by a comma."],"metadata":{"id":"MZWcWpR0GB9f"},"id":"MZWcWpR0GB9f"},{"cell_type":"code","source":["tensor = torch.ones(4, 4)\n","tensor[:,1] = 0 # set second col to 0 (remember 0-indexing in Python)\n","print(tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sICMyTJTGDvg","executionInfo":{"status":"ok","timestamp":1744884038500,"user_tz":-120,"elapsed":66,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"e902a906-7d3e-475f-843e-15edf14c6d1d"},"id":"sICMyTJTGDvg","execution_count":161,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}]},{"cell_type":"code","source":["tensor[0] # access 0th element of the tensor, which for a 2D tensor is the first row"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0_wcOlKKMLA","executionInfo":{"status":"ok","timestamp":1744884038509,"user_tz":-120,"elapsed":9,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"f0e752ed-85c0-4304-c89c-0ec111d9dc1f"},"id":"C0_wcOlKKMLA","execution_count":162,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 0., 1., 1.])"]},"metadata":{},"execution_count":162}]},{"cell_type":"markdown","source":["The indexing operations can become more sophisticated:"],"metadata":{"id":"HXdRxN6OLAD4"},"id":"HXdRxN6OLAD4"},{"cell_type":"code","source":["# get the top left element (the 0's in our indexing example) of each element (colon : runs through all elements) in our tensor\n","x = torch.Tensor([\n","                  [[1, 2], [3, 4]],\n","                  [[5, 6], [7, 8]],\n","                  [[9, 10], [11, 12]]\n","                 ])\n","x[:, 0, 0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtI9dcG5K0hZ","executionInfo":{"status":"ok","timestamp":1744884038517,"user_tz":-120,"elapsed":10,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"aba0fac1-d8c9-40a0-c20e-e850c0aa1a56"},"id":"gtI9dcG5K0hZ","execution_count":163,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 5., 9.])"]},"metadata":{},"execution_count":163}]},{"cell_type":"markdown","source":["**Count and access tensor elements**\n","\n","Use `numel()` to count the elements in a tensor."],"metadata":{"id":"5QUOVpNZIsMw"},"id":"5QUOVpNZIsMw"},{"cell_type":"code","source":["i = torch.tensor([1, 2])\n","i.numel()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qaen7tk2I40y","executionInfo":{"status":"ok","timestamp":1744884038525,"user_tz":-120,"elapsed":10,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"fca6b582-880b-4f12-a128-e4887ad53fb7"},"id":"qaen7tk2I40y","execution_count":164,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":164}]},{"cell_type":"markdown","source":["Use `item()` to access a tensor's underlying elements. This works on flattened or single dimensions of a tensor:"],"metadata":{"id":"OsXz-kbfLaR4"},"id":"OsXz-kbfLaR4"},{"cell_type":"code","source":["i[0].item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"srqmKctwLe8k","executionInfo":{"status":"ok","timestamp":1744884038533,"user_tz":-120,"elapsed":9,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"cfd00e90-2a55-4164-c9c1-1a294e8590da"},"id":"srqmKctwLe8k","execution_count":165,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":165}]},{"cell_type":"markdown","source":["**Joining tensors**\n","\n","You can use `torch.cat` to concatenate a sequence of\n","tensors along a given dimension. See also\n","[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html),\n","another tensor joining op that is subtly different from `torch.cat`."],"metadata":{"id":"C4wrKYehGia_"},"id":"C4wrKYehGia_"},{"cell_type":"code","source":["t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(t1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pnfMYO1aGkt4","executionInfo":{"status":"ok","timestamp":1744884038538,"user_tz":-120,"elapsed":4,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"3569d1c2-d1d2-4ea4-93db-5a41439d9a7a"},"id":"pnfMYO1aGkt4","execution_count":166,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"]}]},{"cell_type":"markdown","source":["**Reshaping tensors**\n","\n","We can change the shape of a tensor with `view()` by simply specifying our desired shape:"],"metadata":{"id":"7GZ1XkS9Gp2A"},"id":"7GZ1XkS9Gp2A"},{"cell_type":"code","source":["# x_view shares the same memory as x, so changing one changes the other\n","x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n","print(x) # before reshaping, shape = (3,2)\n","x_view = x.view(2, 3)\n","print(x_view) # after reshaping, shape = (2,3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJx9CCEAGxWx","executionInfo":{"status":"ok","timestamp":1744884038547,"user_tz":-120,"elapsed":8,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"b9a7bc05-a6d4-4beb-c93d-2bceccf99d3b"},"id":"OJx9CCEAGxWx","execution_count":167,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])\n","tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n"]}]},{"cell_type":"markdown","source":["We can also just specify some of the dimensions and leave it up to Pytorch to infer the rest of them. Say we know that we want to have 3 rows, and we don't care how the rest of the tensor is structured, then we specify all dimensions we don't care about with `-1`:"],"metadata":{"id":"S7En8KcYHL9x"},"id":"S7En8KcYHL9x"},{"cell_type":"code","source":["x_view = x.view(3, -1)\n","x_view"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onsy24F8HVjx","executionInfo":{"status":"ok","timestamp":1744884038552,"user_tz":-120,"elapsed":4,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"d72db6f8-494a-4745-ee70-ed92cb063a79"},"id":"onsy24F8HVjx","execution_count":168,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])"]},"metadata":{},"execution_count":168}]},{"cell_type":"markdown","source":["We can remove singular dimensions with the `squeeze()` function."],"metadata":{"id":"_o844I0WIG5m"},"id":"_o844I0WIG5m"},{"cell_type":"code","source":["x = torch.arange(10).reshape(5, 1, 2) # arange creates a list of 0-9 numbers, reshape shapes them into a tensor of dims (5,2)\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YjxHqF30H5-H","executionInfo":{"status":"ok","timestamp":1744884038596,"user_tz":-120,"elapsed":43,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"1ac49105-f78d-4724-f448-a7cb560228f5"},"id":"YjxHqF30H5-H","execution_count":169,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0, 1]],\n","\n","        [[2, 3]],\n","\n","        [[4, 5]],\n","\n","        [[6, 7]],\n","\n","        [[8, 9]]])"]},"metadata":{},"execution_count":169}]},{"cell_type":"code","source":["x = x.squeeze() # removes the extra empty dimension, unsqueeze would add it back\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DT--ZTCuIQig","executionInfo":{"status":"ok","timestamp":1744884038601,"user_tz":-120,"elapsed":19,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"a14eb83d-3597-402c-f7a8-03b3d98875b2"},"id":"DT--ZTCuIQig","execution_count":170,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])"]},"metadata":{},"execution_count":170}]},{"cell_type":"markdown","source":["**Multiplying tensors**\n","\n","Similar to Numpy matrices, we have different ways to multiply Tensors:"],"metadata":{"id":"qQwMBJ2MJDpA"},"id":"qQwMBJ2MJDpA"},{"cell_type":"code","source":["# This computes the element-wise product\n","print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n","# Alternative syntax:\n","print(f\"tensor * tensor \\n {tensor * tensor}\")\n","# Alternative syntax:\n","print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyovqkgSJKJo","executionInfo":{"status":"ok","timestamp":1744884038602,"user_tz":-120,"elapsed":15,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"567ca4ff-1356-42dd-8e23-19602e2737cb"},"id":"GyovqkgSJKJo","execution_count":171,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor.mul(tensor) \n"," tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]]) \n","\n","tensor * tensor \n"," tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n","tensor @ tensor.T \n"," tensor([[3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.],\n","        [3., 3., 3., 3.]])\n"]}]},{"cell_type":"markdown","source":["**In-place operations**\n","\n","In-place operations are operations that modify the datastructure directly, without having to re-assess it.\n","Operations that have a `_` suffix are in-place.\n","For example: `x.add_(y)` will directly add `y` to `x` without needing to call `x = x.add(y)`. However, their use is discouraged when computing derivates (important later when traiing models) due to the loss of the history."],"metadata":{"id":"xq8iafJqJdUJ"},"id":"xq8iafJqJdUJ"},{"cell_type":"code","source":["print(tensor, \"\\n\")\n","tensor.add_(5)\n","print(tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Sy8JJ_MJ1RR","executionInfo":{"status":"ok","timestamp":1744884038602,"user_tz":-120,"elapsed":11,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"9dc6e16c-eaca-48c3-e751-456f040017fa"},"id":"1Sy8JJ_MJ1RR","execution_count":172,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]]) \n","\n","tensor([[6., 5., 6., 6.],\n","        [6., 5., 6., 6.],\n","        [6., 5., 6., 6.],\n","        [6., 5., 6., 6.]])\n"]}]},{"cell_type":"markdown","source":["**From Tensors to NumPy**\n","\n","Just as we can go from Numpy arrays to tensors, we can also convert them back."],"metadata":{"id":"FOI4Z8MzL-QZ"},"id":"FOI4Z8MzL-QZ"},{"cell_type":"code","source":["t = torch.ones(5)\n","print(f\"Tensor: {t}\")\n","n = t.numpy()\n","print(f\"Numpy: {n}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4fwgTHXMHdy","executionInfo":{"status":"ok","timestamp":1744884038609,"user_tz":-120,"elapsed":6,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"3e4b3bf8-64d6-40fa-87e1-9debe5f07723"},"id":"-4fwgTHXMHdy","execution_count":173,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor: tensor([1., 1., 1., 1., 1.])\n","Numpy: [1. 1. 1. 1. 1.]\n"]}]},{"cell_type":"markdown","source":["A change in the tensor reflects a change in the NumPy array:"],"metadata":{"id":"M7TJtaiGMM4a"},"id":"M7TJtaiGMM4a"},{"cell_type":"code","source":["t.add_(1)\n","print(f\"Tensor: {t}\")\n","print(f\"Numpy: {n}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-0n3PF2MSoy","executionInfo":{"status":"ok","timestamp":1744884038619,"user_tz":-120,"elapsed":9,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"241c3081-e164-4a82-8b6b-5a0023421edf"},"id":"T-0n3PF2MSoy","execution_count":174,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor: tensor([2., 2., 2., 2., 2.])\n","Numpy: [2. 2. 2. 2. 2.]\n"]}]}],"metadata":{"kernelspec":{"display_name":"cs224n","language":"python","name":"cs224n"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}